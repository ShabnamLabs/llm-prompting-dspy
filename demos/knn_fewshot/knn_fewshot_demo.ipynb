{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f454a5",
   "metadata": {},
   "source": [
    "# KNN Few-Shot Prompting Demo\n",
    "\n",
    "This notebook demonstrates how to use DSPy with a KNN-based few-shot retriever for dynamic prompt construction.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Setup**: Configure API and DSPy.\n",
    "2. **Load Data**: Load training examples for few-shot retrieval.\n",
    "3. **Compile Pipeline**: Build a DSPy pipeline using KNN retrieval.\n",
    "4. **Inference**: Query the pipeline with test questions.\n",
    "5. **Inspect Prompt**: View the selected few-shot examples for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac0ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary functions and libraries\n",
    "from src.knn_fewshot_pipeline import load_examples, compile_knn_dspy_pipeline, inference_knn_dspy_pipeline, QuestionAnswer\n",
    "import dspy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"  # Replace with your actual OpenAI API key\n",
    "dspy.settings.configure(lm=dspy.LM(\"openai/gpt-4o-mini\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a75e66",
   "metadata": {},
   "source": [
    "## Build and compile the KNN few-shot prompting pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6de3afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_path = \"data/examples.json\"\n",
    "embedder_model_name = \"all-MiniLM-L6-v2\"\n",
    "k = 3\n",
    "\n",
    "# Load training examples\n",
    "trainset = load_examples(example_path)\n",
    "\n",
    "# Create DSPy module for question answering\n",
    "qa_module = dspy.Predict(QuestionAnswer)\n",
    "\n",
    "# Compile the KNN few-shot pipeline\n",
    "compiled_qa = compile_knn_dspy_pipeline(trainset, qa_module, k, embedder_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825e9ff",
   "metadata": {},
   "source": [
    "## Query on a test example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3d298",
   "metadata": {},
   "source": [
    "### Example 1: Capital of Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0195d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 17.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 2 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Q: What is the capital of Belgium?\n",
      "A: Prediction(\n",
      "    answer='Brussels'\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of Belgium?\"\n",
    "answer = inference_knn_dspy_pipeline(compiled_qa, question=question)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04a25c",
   "metadata": {},
   "source": [
    "#### Few-shot examples selected for this prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7760f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': None, 'messages': [{'role': 'system', 'content': 'Your input fields are:\\n1. `question` (str): input text to answer the question\\nYour output fields are:\\n1. `answer` (str): the answer to the question\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## answer ## ]]\\n{answer}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Answer the question.'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nWhat is the capital of France?'}, {'role': 'assistant', 'content': '[[ ## answer ## ]]\\nParis\\n\\n[[ ## completed ## ]]\\n'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nWhat language is primarily spoken in Brazil?'}, {'role': 'assistant', 'content': '[[ ## answer ## ]]\\nPortuguese\\n\\n[[ ## completed ## ]]\\n'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nWhat is the currency of Japan?'}, {'role': 'assistant', 'content': '[[ ## answer ## ]]\\nYen\\n\\n[[ ## completed ## ]]\\n'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nWhat is the capital of Belgium?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}], 'kwargs': {}, 'response': ModelResponse(id='chatcmpl-C4YMnJvVU97h12z7mPS2cn59FB7Gt', created=1755201453, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_1542007cca', choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## answer ## ]]\\nBrussels\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default'), 'outputs': ['[[ ## answer ## ]]\\nBrussels\\n\\n[[ ## completed ## ]]'], 'usage': {}, 'cost': 4.485e-05, 'timestamp': '2025-08-18T19:10:26.682945', 'uuid': '3d5f5b08-cfb2-4b12-af47-20c9f038d05f', 'model': 'openai/gpt-4o-mini', 'response_model': 'gpt-4o-mini-2024-07-18', 'model_type': 'chat'}\n"
     ]
    }
   ],
   "source": [
    "print(compiled_qa.history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749eba6",
   "metadata": {},
   "source": [
    "### Example 2: Language in Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d03d4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 21.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 2 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Q: What language is primarily spoken in Japan?\n",
      "A: Prediction(\n",
      "    answer='Japanese'\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What language is primarily spoken in Japan?\"\n",
    "answer = inference_knn_dspy_pipeline(compiled_qa, question=question)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ffdadf",
   "metadata": {},
   "source": [
    "#### Few-shot examples selected for this prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ee6ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': None, 'messages': [{'role': 'system', 'content': 'Your input fields are:\\n1. `question` (str): input text to answer the question\\nYour output fields are:\\n1. `answer` (str): the answer to the question\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## answer ## ]]\\n{answer}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Answer the question.'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nWhat language is primarily spoken in Brazil?'}, {'role': 'assistant', 'content': '[[ ## answer ## ]]\\nPortuguese\\n\\n[[ ## completed ## ]]\\n'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nWhat is the currency of Japan?'}, {'role': 'assistant', 'content': '[[ ## answer ## ]]\\nYen\\n\\n[[ ## completed ## ]]\\n'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nWhat is the largest ocean on Earth?'}, {'role': 'assistant', 'content': '[[ ## answer ## ]]\\nPacific Ocean\\n\\n[[ ## completed ## ]]\\n'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nWhat language is primarily spoken in Japan?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}], 'kwargs': {}, 'response': ModelResponse(id='chatcmpl-C4YMrHpiYHOx2glyLWwCJxIs9igQd', created=1755201457, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## answer ## ]]\\nJapanese\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default'), 'outputs': ['[[ ## answer ## ]]\\nJapanese\\n\\n[[ ## completed ## ]]'], 'usage': {}, 'cost': 4.4699999999999996e-05, 'timestamp': '2025-08-18T19:10:31.384686', 'uuid': 'd1b1db9c-73cf-4dbb-88a1-a87320c630ee', 'model': 'openai/gpt-4o-mini', 'response_model': 'gpt-4o-mini-2024-07-18', 'model_type': 'chat'}\n"
     ]
    }
   ],
   "source": [
    "print(compiled_qa.history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9073ffcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
